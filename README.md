
## ðŸ“– AwesomePaper & History behind the key concept of LLMs 

| Date | Paper | Description | Key Insights & Contributions |
|---|---|---|---|
| December 1998 | LeNet | Introduced convolutional neural networks (CNNs)  | Pioneered the use of convolutions for image processing, laying the groundwork for modern CNN architectures. |
| September 2012 | AlexNet | Introduced ReLU activation and Dropout to CNNs. | Showcased the effectiveness of ReLU activation for faster training and Dropout for reducing overfitting. |
| September 2014 | VGG | Used a large number of small-sized filters in each layer to learn complex features. | Demonstrated that depth and smaller filter sizes could significantly improve performance in image recognition tasks. |
| September 2014 | Inception Net | Introduced Inception Modules consisting of multiple parallel convolutional layers. | Pioneered multi-scale feature extraction by processing input images at various resolutions. |
| December 2015 | Inception Net v2 / v3 | Design optimizations of Inception Modules. | Introduced architectural improvements such as factorization and batch normalization, enhancing efficiency and accuracy. | 
| December 2015 | ResNet | Introduced residual connections (skip connections). | Revolutionized deep learning by enabling the training of much deeper networks, significantly boosting performance. |
| February 2016 | Inception Net v4 / Inception-ResNet | A hybrid approach combining Inception Net and ResNet. | Demonstrated the benefits of combining multi-scale processing with residual connections for further performance improvements. |
| April 2016 | Entity Embeddings | Maps categorical variables into continuous vector spaces. | Pioneered representation learning for tabular data, enabling neural networks to handle categorical features effectively. | 
| June 2016 | Wide & Deep Learning | Combines memorization of specific patterns with generalization of similarities. | Introduced a model architecture that balances the strengths of linear models (memorization) and deep networks (generalization). |
| August 2016 | DenseNet | Each layer receives input from all preceding layers. | Showcased a densely connected network architecture that encourages feature reuse and improves information flow. |
| October 2016 | Xception | Based on InceptionV3, but uses depthwise separable convolutions. | Improved upon Inception by using a more efficient convolution operation, leading to better performance and reduced computational cost. |
| November 2016 | ResNeXt | Built over ResNet, introduces grouped convolutions. | Further enhanced ResNet by dividing filters into groups, allowing the network to learn more diverse features. |
| June 2017 | Transformer | Introduced the multi-head attention mechanism. | Marked a paradigm shift in NLP by replacing recurrent networks with attention, enabling parallel processing and improved long-range dependencies. |
| April 2017 | MobileNet V1 | Uses depthwise separable convolutions to reduce parameters and computation. | Introduced an efficient mobile-friendly CNN architecture that achieved impressive performance on resource-constrained devices. |
| August 2017 | Deep & Cross Network (DCN) | Combines a novel cross network with deep neural networks. | Effectively learns feature interactions in tabular data without manual feature engineering, enhancing the model's expressiveness. | 
| January 2018 | MobileNet V2 | Built upon MobileNet V1, uses inverted residuals and linear bottlenecks. | Further enhanced the MobileNet architecture, improving accuracy and efficiency for mobile applications. |
| February 2018 | ELMo | Introduced deep contextualized word representations. | Captured nuanced word meanings based on surrounding context, significantly improving NLP tasks that rely on word understanding. |
| June 2018 | GPT |  A decoder-only transformer pre-trained autoregressively. | Showcased the power of autoregressive pre-training for various downstream NLP tasks, paving the way for future GPT models. |
| October 2018 | BERT | Introduced pre-training for encoder transformers. | Revolutionized NLP by demonstrating the effectiveness of bi-directional pre-training, significantly boosting performance on many tasks. |
| January 2019 | Transformer-XL | Extends the transformer to handle longer sequences. | Introduced recurrence into the self-attention mechanism, enabling the model to process longer text sequences and capture dependencies. | 
| February 2019 | GPT-2 | Demonstrated unsupervised learning of various NLP tasks. | Showed that large language models can learn diverse tasks without explicit supervision, suggesting emergent capabilities. |
| May 2019 | UniLM | Utilizes a shared Transformer network with specific self-attention masks. | Excels in both language understanding and generation tasks by adapting the transformer architecture with task-specific masks. |
| May 2019 | MobileNet V3 | Uses AutoML to find the best neural network architecture. |  Leveraged automated machine learning (AutoML) to optimize the MobileNet architecture for even better performance and efficiency. | 
| May 2019 | EfficientNet | Uses a compound scaling method to scale the network's depth, width, and resolution. |  Introduced a systematic approach for scaling CNNs, achieving high accuracy with minimal computational cost. |
| June 2019 | XLNet | Extension of Transformer-XL, pre-trained using a combination of AR and AE objectives. | Further improved upon Transformer-XL by using a novel pre-training objective that combines autoregressive and autoencoding approaches. |
| July 2019 | RoBERTa | Optimized hyperparameters and training data size for BERT. | Showed that carefully tuning the training process could significantly improve BERT's performance on various NLP tasks. |
| August 2019 | Sentence-BERT | A BERT modification using siamese and triplet network structures. |  Introduced a technique for deriving sentence embeddings that are effective for tasks like semantic similarity and clustering. |
| September 2019 | TinyBERT |  Uses attention transfer and task-specific distillation. |  Presented a method for compressing BERT into a smaller model while retaining high performance through knowledge distillation. |
| September 2019 | ALBERT | Presents parameter reduction techniques for BERT. | Introduced efficient ways to reduce memory consumption and increase training speed for large language models. |
| October 2019 | DistilBERT | Distills BERT on large batches with dynamic masking. |  Showcased an efficient approach to distilling BERT into a smaller model while maintaining good performance. |
| October 2019 | T5 | Unifies text-based language problems into a text-to-text format. | Simplified NLP task formulation and training by converting various tasks into a single text-to-text format. | 
| October 2019 | BART |  An encoder-decoder pre-trained to reconstruct text from corrupted versions. |  Introduced a model pre-trained on a denoising task, enabling it to perform well on both language understanding and generation. |
| December 2019 | LayoutLM | Utilises BERT and adds 2D position and image embeddings. | Pioneered the use of layout information for document understanding tasks, enhancing BERT with spatial awareness. |
| February 2020 | UniLMv2 | Utilizes a pseudo-masked language model (PMLM). | Enhanced the UniLM model for improved performance on a broader range of NLP tasks. |
| February 2020 | LamBERT | Utilises RoBERTa and adds layout embeddings and relative bias. | Improved upon LayoutLM by incorporating layout information during pre-training and using a more powerful RoBERTa backbone. |
| January 2020 | TableNet |  An end-to-end deep learning model for table detection and structure recognition. |  Introduced a model for automatically extracting tables from documents and understanding their structure. |
| April 2020 | FastBERT |  A speed-tunable encoder with adaptive inference time. |  Enabled faster inference by allowing the model to make early predictions based on confidence levels. | 
| April 2020 | MobileBERT | A compressed and faster version of BERT. |  Optimized BERT for mobile devices, enabling efficient NLP on resource-constrained platforms. |
| April 2020 | Longformer | Introduces a linearly scalable attention mechanism. |  Enabled processing long text sequences by replacing the standard quadratic attention with a more efficient linear attention. |
| April 2020 | Dense Passage Retriever (DPR) |  Shows that retrieval can be implemented using dense representations. |  Demonstrated the effectiveness of dense embeddings for information retrieval tasks, simplifying traditional methods. |
| April 2020 | ColBERT |  Adapts deep language models for efficient retrieval using late interaction. |  Introduced a retrieval model that combines the power of deep language models with an efficient late interaction architecture. |
| May 2020 | GPT-3 | Demonstrates significant improvements in few-shot learning with scale. | Showed that massively increasing the size of language models leads to impressive gains in task-agnostic performance. |
| May 2020 | DETR |  Treats object detection as a set prediction problem. |  Simplified object detection by eliminating hand-designed components and using transformers for direct set prediction. |
| June 2020 | DeBERTa | Enhances BERT with disentangled attention, enhanced mask decoder, and adversarial training. | Improved BERT's performance through architectural and training enhancements, achieving state-of-the-art results on various tasks. |
| June 2020 | DeBERTa v2 | Enhanced version of DeBERTa. |  Further improved DeBERTa with a new vocabulary, optimized attention, and additional model sizes, achieving even better results. |
| June 2021 | DocFormer | Encoder-only transformer with a CNN backbone for visual feature extraction. | Introduced a model for document understanding that effectively combines text, vision, and spatial features. |
| June 2021 | Tabular ResNet | An MLP with skip connections for tabular data. | Adapted the ResNet architecture for tabular data, enhancing the model's ability to learn complex relationships. | 
| June 2021 | Feature Tokenizer Transformer | Transforms all features to embeddings and applies Transformer layers. | Introduced a transformer-based approach for tabular data that effectively handles both categorical and numerical features. |
| July 2020 | T5 v1.1 |  An enhanced version of the original T5 model. |  Introduced several improvements to the T5 architecture and training process, boosting its performance on various tasks. |
| July 2021 | Codex | A GPT language model fine-tuned on code from GitHub. |  Demonstrated the ability of LLMs to generate code, paving the way for AI-assisted programming tools. |
| September 2021 | FLAN | An instruction-tuned language model fine-tuned on various NLP datasets with natural language instructions. |  Showcased the effectiveness of instruction tuning for improving the model's ability to follow instructions and perform diverse tasks. | 
| October 2020 | Vision Transformer (ViT) |  Segments images into patches, treats them as tokens, and inputs them to a Transformer. |  Pioneered the application of transformers to image recognition, achieving competitive results with convolutional approaches. | 
| October 2020 | mT5 | A multilingual variant of T5 pre-trained on a dataset covering 101 languages (mC4). |  Extended the T5 model to the multilingual domain, enabling it to perform tasks in multiple languages. |
| October 2021 | T0 |  A fine-tuned encoder-decoder model achieving strong zero-shot performance. | Demonstrated that multi-task training could lead to impressive zero-shot performance on various NLP tasks. |
| October 2021 | MobileViT |  A lightweight vision transformer designed for mobile devices. |  Combined the strengths of CNNs and transformers to create an efficient and accurate model for mobile vision tasks. | 
| November 2021 | Donut |  An OCR-free encoder-decoder transformer model for document understanding. |  Introduced a model that can understand documents without relying on optical character recognition (OCR). |
| November 2021 | Masked Autoencoder (MAE) |  An encoder-decoder architecture that reconstructs images by masking random patches. | Introduced a self-supervised pre-training method for vision models, achieving impressive results with a simple masking technique. |
| December 2020 | LayoutLM v2 |  Uses a multi-modal transformer to integrate text, layout, and image information. |  Further enhanced the LayoutLM model by integrating image information during pre-training, improving performance on document understanding. |
| December 2021 | ColBERTv2 | Improves the quality and space footprint of late interaction for retrieval. |  Enhanced the ColBERT retrieval model with efficient compression and denoising techniques, improving accuracy and efficiency. |
| December 2021 | Gopher |  Provides a comprehensive analysis of transformer performance across scales. |  Offered valuable insights into the scaling behavior of transformers, showing how performance changes with model size and training data. |
| December 2021 | WebGPT |  A fine-tuned GPT-3 model trained to use text-based web browsing. |  Demonstrated the potential of combining language models with web browsing capabilities for more comprehensive information access. |
| January 2022 | LaMDA |  Transformer models specialized for dialogue. |  Focused on building conversational AI models that can engage in more natural and coherent dialogues. |
| January 2022 | ConvMixer |  Processes image patches using standard convolutions for spatial and channel mixing. |  Introduced a simple yet effective CNN architecture that achieves surprisingly good performance in image recognition tasks. |
| January 2022 | ConvNeXt |  A pure ConvNet model competing with transformers in accuracy and scalability. |  Showed that convolutional networks can still compete with transformers in terms of accuracy and scalability. |
| February 2021 | CLIP | A vision system that learns image representations from text-image pairs. |  Pioneered contrastive learning for vision-language tasks, enabling zero-shot transfer to various downstream tasks. |
| February 2022 | LiLT | Introduced Bi-directional attention complementation mechanism (BiACM) for text and layout interaction. |  Further enhanced the integration of text and layout information for document understanding tasks. |
| March 2022 | InstructGPT |  Fine-tuned GPT using instruction tuning and human feedback. |  Showcased the effectiveness of aligning language models with user intent through instruction tuning and human feedback. |
| March 2022 | CodeGen |  An LLM trained for program synthesis using examples and descriptions. |  Advanced the field of AI code generation by training a model specifically for synthesizing programs from natural language instructions. |
| March 2022 | Chinchilla | Investigated optimal model size and training data for transformers. |  Provided important insights into the scaling laws of language models, guiding researchers towards more efficient training strategies. | 
| April 2022 | PaLM | A 540B parameter transformer trained using the Pathways system. |  Introduced the Pathways system for efficient distributed training of very large language models. |
| April 2022 | GPT-NeoX-20B | An autoregressive LLM trained on The Pile dataset. |  Showcased the capabilities of large language models trained on a massive and diverse text dataset. | 
| April 2022 | Flamingo | Visual Language Models enabling seamless handling of interleaved visual and textual data. |  Pioneered the development of large multimodal models that can process both visual and textual information, paving the way for more sophisticated vision-language tasks. | 
| April 2022 | LayoutLM v3 | A unified text-image multimodal transformer for cross-modal representations. |  Combined text and image information during pre-training, further improving performance on document understanding tasks. |
| May 2022 | OPT | A suite of decoder-only pre-trained transformers with parameter ranges from 125M to 175B. |  Made large language models more accessible by releasing a series of open-source models with varying sizes. | 
| May 2022 | Matryoshka Representation Learning | Encodes information at different granularities. | Introduced a flexible representation learning technique that allows adapting to various tasks with varying computational resources. |
| June 2022 | DeBERTa v3 |  A further enhanced version of DeBERTa, incorporating new techniques and pre-training objectives. | Continued the advancement of the DeBERTa model, achieving state-of-the-art results on a wide range of NLP benchmarks. |
| October 2022 | Flan T5, Flan PaLM | Explores instruction fine-tuning with a focus on scaling tasks, model size, and chain-of-thought data. | Showcased the effectiveness of combining instruction tuning with various techniques like scaling and chain-of-thought prompting. |
| October 2022 | ERNIE Layout | Reorganizes tokens using layout information, combines text and visual embeddings. |  Further enhanced the integration of layout information for document understanding tasks, improving performance on tasks like form understanding. | 
| November 2022 | BLOOM |  A 176B parameter, open-access decoder-only transformer. |  A collaborative effort to democratize access to large language models, making them available for research and development. |
| November 2022 | BLOOMZ, mT0 |  Applies multitask prompted fine-tuning to pretrained multilingual models. |  Explored techniques for adapting multilingual language models to perform tasks in various languages through prompting. |
| November 2022 | Galactica |  An LLM trained on scientific data, specializing in scientific knowledge. |  Demonstrated the potential of training language models on specialized domains to achieve high performance on specific tasks. |
| November 2022 | ChatGPT |  An interactive model designed for engaging in conversations. |  Showcased the impressive capabilities of conversational AI, capturing the attention of the public and researchers alike. |
| December 2022 | Self-Instruct |  A framework for improving instruction-following capabilities through bootstrapping. |  Introduced a technique for generating instruction data automatically, reducing the need for manual annotation and improving instruction-following abilities. |
| December 2022 | E5 |  A family of text embeddings trained with weak supervision from text pairs. |  Introduced a method for learning effective text representations using contrastive learning and a large-scale dataset of text pairs. | 
| December 2022 | ColD Fusion |  Enables multitask learning through distributed computation without data sharing. |  Introduced a novel technique for achieving the benefits of multitask learning without the need to share sensitive data across different parties. |
| December 2022 | UDoP | Integrates text, image, and layout information through a Vision-Text-Layout Transformer. |  Advanced document understanding by effectively combining text, image, and layout information in a single model. |
| January 2023 | ConvNeXt V2 |  Incorporates a fully convolutional MAE framework and a Global Response Normalization (GRN) layer. |  Further enhanced the ConvNeXt architecture, achieving state-of-the-art results on multiple image recognition benchmarks. |
| January 2024 | DocLLM | A lightweight extension to traditional LLMs that focuses on reasoning over visual documents. |  Enabled reasoning over visual documents by incorporating textual semantics and spatial layout without expensive image encoders. |
| January 2024 | TinyLlama |  A 1.1B parameter language model based on Llama 2, pre-trained on 1 trillion tokens. |  Showcased the effectiveness of training smaller language models with efficient techniques like FlashAttention and Grouped Query Attention. |
| January 2024 | Mixtral 8x7B |  A Sparse Mixture of Experts language model trained with multilingual data using a 32k context size. |  Introduced a multilingual model with a large context size, combining sparsity techniques for efficiency. |
| January 2024 | H2O Danube 1.8B |  A language model trained on 1T tokens following the principles of LLama 2 and Mistral. |  Showcased a well-performing open-source language model based on recent advancements in training techniques. |
| February 2023 | LLaMA |  A collection of foundation LLMs trained on publicly available datasets. |  Made powerful large language models more accessible to researchers and developers by releasing open-weight models. |
| February 2023 | Toolformer |  An LLM trained to decide which APIs to call and how to best incorporate the results. |  Pioneered the integration of language models with external tools, enhancing their capabilities for real-world tasks. |
| February 2024 | Nomic Embed Text v1 |  A 137M parameter, open-source English text embedding model with an 8192 context length. |  Introduced an efficient and effective text embedding model that outperforms larger models on both short and long context tasks. |
| February 2024 | Nomic Embed Text v1.5 |  An advanced text embedding model utilizing Matryoshka Representation Learning. |  Extended the Nomic Embed model with Matryoshka representations, offering flexible embedding sizes with minimal performance trade-offs. |
| February 2024 | Gemma |  A family of 2B and 7B state-of-the-art language models based on Google's Gemini models. |  Showcased advancements in language understanding, reasoning, and safety, further pushing the capabilities of open-source models. | 
| February 2024 | Aya 101 |  A massively multilingual generative language model that follows instructions in 101 languages. |  Bridged the language gap by introducing a multilingual model trained on a diverse dataset covering numerous languages. |
| February 2024 | Hawk, Griffin | Introduces Real Gated Linear Recurrent Unit Layer to replace Multi-Query Attention. |  Enhanced the efficiency and scalability of language models by introducing a novel recurrent block based on a gated linear recurrent unit. | 
| February 2024 | OLMo |  A state-of-the-art, truly open language model and framework including training data, code, and tools. |  Furthered the open-source movement in language modeling by providing a complete package for researchers and developers to build upon. |
| March 2023 | Alpaca |  A fine-tuned LLaMA 7B model trained on instruction-following demonstrations. |  Demonstrated the effectiveness of fine-tuning smaller language models on instruction-following data to achieve high performance. | 
| March 2023 | GPT-4 |  A multimodal transformer accepting image and text inputs and producing text outputs. |  Significantly expanded the capabilities of language models by enabling them to process and reason over both text and images. |
| March 2023 | Vicuna |  A 13B LLaMA chatbot fine-tuned on user-shared conversations. |  Showcased the potential of fine-tuning language models on real-world dialogue data to create more engaging and informative chatbots. |
| March 2023 | BloombergGPT |  A 50B language model trained on general-purpose and financial data. |  Demonstrated the value of domain-specific training for language models, achieving impressive results on financial tasks. | 
| March 2024 | WRAP |  Uses an off-the-shelf instruction-tuned model to paraphrase documents in various styles. |  Introduced a method for pre-training LLMs using synthetic data generated through paraphrasing, enhancing their ability to understand and generate different writing styles. | 
| March 2024 | DBRX |  A 132B, open, general-purpose, fine-grained Sparse MoE LLM. |  Showcased the potential of Sparse Mixture of Experts (MoE) models for achieving high performance with efficient resource utilization. |
| March 2024 | LLMLingua2 |  A task-agnostic approach to prompt compression, enhancing generalizability using data distillation. |  Introduced a more generalizable prompt compression technique that can be applied to various tasks, improving efficiency across different domains. | 
| April 2023 | Pythia |  A suite of 16 LLMs trained on public data, ranging in size from 70M to 12B parameters. |  Provided a valuable resource for researchers to study the scaling behavior and performance of language models across different sizes. | 
| April 2023 | LLaVA 1 |  A large multimodal model connecting CLIP and Vicuna, trained on instruction-following data generated by GPT-4. |  Further advanced the development of multimodal models, enabling them to follow instructions and generate responses based on both image and text inputs. |
| April 2023 | WizardLM |  Introduces Evol-Instruct, a method for generating instruction data using LLMs. |  Introduced a novel method for automatically creating large-scale instruction data, reducing the need for manual annotation and enabling more efficient model training. |
| April 2024 | CodeGemma |  Open code models based on Gemma, trained on over 500 billion tokens of code. |  Extended the capabilities of the Gemma models to the code domain, enabling them to generate high-quality code in multiple programming languages. | 
| April 2024 | RecurrentGemma | Based on Griffin, combines linear recurrences and local attention for efficient long sequence modeling. | Introduced a more efficient approach to processing long sequences, combining the benefits of linear recurrences and local attention mechanisms. | 
| April 2024 | Rho-1 | Introduces Selective Language Modeling, optimizing loss on tokens aligning with a desired distribution. |  Introduced a novel training technique that focuses on optimizing the model's performance on specific types of tokens, enhancing its ability to generate text with desired characteristics. |
| April 2024 | Phi-3 | A series of language models trained on filtered web and synthetic data, achieving comparable performance to larger models. |  Demonstrated that carefully selecting and curating training data can lead to impressive performance even with smaller model sizes. |
| April 2024 | OpenELM | A fully open language model with layer-wise scaling for enhanced accuracy and efficiency. | Introduced a novel architecture that scales model dimensions layer-wise, achieving high accuracy with fewer parameters and training tokens. |
| April 2024 | H2O Danube2 1.8B |  An updated version of the original H2O-Danube model with improved performance. |  Showcased the continuous development of open-source language models, incorporating refinements and optimizations for better results. |
| May 2023 | CodeGen2 |  Proposes a unified approach for training LLMs for program synthesis. |  Improved the efficiency of training code generation models by unifying key components of model architecture, learning methods, and data distributions. | 
| May 2023 | PaLM 2 |  Successor to PaLM, trained on a mixture of pre-training objectives for a deeper understanding of language. |  Showcased a more comprehensive and versatile language model trained on multiple objectives, improving performance across diverse tasks. |
| May 2023 | LIMA |  A LLaMA model fine-tuned on only 1,000 carefully curated prompts and responses. |  Demonstrated that high performance can be achieved with smaller models and limited data through careful prompt engineering and data selection. | 
| May 2023 | Gorilla |  A retrieve-aware fine-tuned LLaMA-7B model for API calls. |  Enhanced the ability of language models to interact with external APIs, enabling them to access and utilize external information more effectively. |
| May 2024 | Granite Code Models | A family of code models trained on 3.5-4.5T tokens of code in 116 programming languages. |  Pushed the boundaries of code generation by training models on a massive dataset covering a wide range of programming languages, enabling them to generate more accurate and diverse code. | 
| May 2024 | Phi-3 Vision | The first multimodal model in the Phi family, capable of reasoning over images. |  Extended the Phi series to the multimodal domain, enabling it to understand and reason over images in addition to text. |
| May 2024 | Gemini 1.5 Flash | A lightweight version of Gemini 1.5 Pro, optimized for efficiency. |  Introduced a more resource-efficient version of the Gemini model, making it suitable for deployment on devices with limited compute power. |
| May 2024 | Chameleon | A family of mixed-modal models capable of understanding and generating images and text in arbitrary sequences. |  Introduced a flexible model architecture that can handle both text and image inputs in any order, enabling more natural and creative multimodal interactions. | 
| June 2023 | Falcon |  An open-source LLM trained on filtered and deduplicated web data. |  Showcased a high-performing language model trained on a massive and carefully curated dataset of web text. |
| June 2023 | Phi-1 |  An LLM for code, trained on textbook quality data and synthetically generated content. |  Demonstrated the effectiveness of using high-quality, curated data for training code generation models, leading to improved performance. |
| June 2023 | WizardCoder |  Enhances the performance of StarCoder using Code Evol-Instruct. |  Further improved the code generation capabilities of open-source models by leveraging Evol-Instruct for data augmentation. |
| July 2023 | LLaMA 2 |  Successor to LLaMA, with models optimized for dialogue. |  Enhanced the LLaMA models with improved architectures and training techniques, further pushing the boundaries of open-weight LLMs. |
| July 2023 | Tool LLM |  A LLaMA model fine-tuned on an instruction-tuning dataset for tool use. |  Specialized the LLaMA model for interacting with external tools, expanding its capabilities for real-world tasks and information access. | 
| August 2023 | Humpback |  LLaMA fine-tuned using instruction backtranslation. |  Introduced a novel fine-tuning technique that leverages backtranslation to improve the model's instruction-following abilities. | 
| August 2023 | Code LLaMA |  An LLaMA 2-based LLM for code generation. |  Specialized the powerful LLaMA 2 model for code generation, achieving impressive results on coding tasks. |
| August 2023 | WizardMath |  Enhances mathematical reasoning abilities of LLaMA-2 using RLEIF. |  Introduced a novel training method that combines reinforcement learning with Evol-Instruct feedback to improve the model's mathematical reasoning capabilities. | 
| September 2023 | LLaMA 2 Long |  A series of long context LLMs supporting context windows up to 32,768 tokens. |  Extended the context window of the LLaMA 2 models, enabling them to process and generate longer and more coherent text. |
| September 2023 | GPT-4V |  A multimodal model allowing users to instruct it to analyze images. |  Expanded the capabilities of GPT-4 to the visual domain, enabling it to understand and analyze images based on user instructions. |
| September 2023 | Phi-1.5 |  Follows the Phi-1 approach, focusing on common sense reasoning in natural language. |  Showcased a smaller model that achieves impressive common sense reasoning capabilities through careful data selection and training techniques. | 
| October 2023 | Mistral 7B | Leverages grouped-query attention and sliding window attention for efficient long sequence processing. |  Introduced architectural innovations to achieve fast and efficient processing of long text sequences with reduced computational cost. | 
| October 2023 | LLaVA 1.5 |  An enhanced version of the LLaVA model, incorporating a larger CLIP model and academic-task-oriented VQA data. |  Further improved the performance and capabilities of the LLaVA model on vision-language tasks, setting new benchmarks in the field. | 
| October 2023 | LLMLingua |  A novel coarse-to-fine prompt compression method for efficient language model interaction. |  Introduced a technique for compressing prompts, reducing the computational cost of interacting with large language models while maintaining performance. |
| October 2023 | LongLLMLingua |  A novel approach for prompt compression in long context scenarios. |  Specialized the prompt compression technique for long context scenarios, improving efficiency and performance when processing large amounts of text. |
| October 2023 | Llemma |  An LLM for mathematics, pre-trained on scientific papers, web data, and mathematical code. |  Demonstrated the potential of training language models on specialized data to achieve high performance on specific domains like mathematics. | 
| October 2023 | CodeFusion |  A diffusion code generation model that iteratively refines programs based on encoded natural language. |  Introduced a novel approach to code generation using diffusion models, enabling more flexible and iterative program refinement. |
| October 2023 | Zephyr 7B |  Utilizes dDPO and AI Feedback (AIF) to achieve superior intent alignment in chat-based language modeling. |  Focused on aligning the model's responses with user intent in chat-based interactions, enhancing the user experience and conversational flow. | 
| December 2023 | Phi-2 |  A 2.7B model exploring emergent abilities in smaller models through strategic training choices. |  Investigated the factors that contribute to emergent abilities in language models, showing that smaller models can achieve impressive performance with careful data selection and training. | 
| December 2023 | Gemini 1.0 |  A family of multi-modal models trained jointly across image, audio, video, and text data. |  Showcased Google's advancements in multimodal AI, introducing a family of models capable of understanding and generating content across various modalities. |
| December 2023 | E5 Mistral 7B | Leverages proprietary LLMs to generate synthetic data for fine-tuning open-source LLMs on text embedding tasks. |  Introduced a method for improving the performance of open-source language models on text embedding tasks by leveraging proprietary models for data generation. |
| December 2023 | An In-depth Look at Gemini's Language Abilities |  A third-party comparison of OpenAI GPT and Google Gemini models. |  Provided a comprehensive and objective evaluation of the capabilities of leading language models from OpenAI and Google. | 
| January 2024 | MoE-LLaVA |  A Mixture-of-Experts based sparse LVLM framework for efficient multimodal modeling. |  Introduced a more efficient multimodal model architecture using Mixture-of-Experts, activating only relevant experts for specific tasks. |
| January 2024 | LLaVA 1.6 |  An improved version of LLaVA 1.5 with enhanced reasoning, OCR, and world knowledge capabilities. |  Further enhanced the LLaVA model with improved reasoning abilities, optical character recognition, and increased world knowledge, expanding its capabilities for complex vision-language tasks. | 
| January 2024 | Dolma |  An open corpus of three trillion tokens for language model pre-training research. |  Provided a massive and open dataset to support research on pre-training large language models, enabling the development of more powerful and versatile models. |
| February 2024 | Gemini 1.5 Pro |  A compute-efficient multimodal mixture-of-experts model excelling in long-context retrieval tasks. |  Introduced a more efficient and powerful multimodal model capable of handling long contexts and understanding across text, video, and audio modalities. |
| March 2024 | MM1 | A multimodal LLM combining a ViT-H image encoder with a 378x378px resolution. |  Showcased a high-performing multimodal model with a larger image encoder, improving its ability to process and understand visual information. |
| May 2024 | H2O Danube2 1.8B | An improved version of H2O-Danube, removing sliding window attention and incorporating data adjustments. |  Showcased the ongoing development and refinement of open-source language models, incorporating lessons learned and new techniques for enhanced performance. | 


This timeline captures a selection of impactful papers, demonstrating the remarkable advancements in the field. Continuous exploration will unveil the ever-evolving landscape of language model research. 
